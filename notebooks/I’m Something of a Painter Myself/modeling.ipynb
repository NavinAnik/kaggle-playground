{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T14:02:22.562731Z",
     "start_time": "2025-06-06T14:02:22.559196Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import glob\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:02:23.157401Z",
     "start_time": "2025-06-06T14:02:23.150619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator: ResNet style\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, n_residual_blocks=9):\n",
    "        super().__init__()\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, output_nc, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator: PatchGAN\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc):\n",
    "        super().__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        in_channels = 64\n",
    "        for i in range(3):\n",
    "            out_channels = in_channels * 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_channels, out_channels, 4, stride=2 if i < 2 else 1, padding=1),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "        model += [nn.Conv2d(in_channels, 1, 4, padding=1)]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "id": "66212464d20e2ccf",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:02:23.620827Z",
     "start_time": "2025-06-06T14:02:23.617250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MonetDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.jpg')))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n"
   ],
   "id": "79109f12bfb98559",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:02:24.070706Z",
     "start_time": "2025-06-06T14:02:24.018801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "photo_path = \"/Users/navin/Documents/Anik/GitHub/kaggle-playground/notebooks/I’m Something of a Painter Myself/datasets/photo_jpg/\"\n",
    "monet_path = \"/Users/navin/Documents/Anik/GitHub/kaggle-playground/notebooks/I’m Something of a Painter Myself/datasets/monet_jpg/\"\n",
    "\n",
    "photo_dataset = MonetDataset(photo_path, transform=transform)\n",
    "monet_dataset = MonetDataset(monet_path, transform=transform)\n",
    "\n",
    "photo_loader = DataLoader(photo_dataset, batch_size=1, shuffle=True)\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=1, shuffle=True)"
   ],
   "id": "8e651faa6fee4c8d",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T14:02:24.429880Z",
     "start_time": "2025-06-06T14:02:24.427731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ],
   "id": "ab42e6c5cf234c8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T17:56:39.397338Z",
     "start_time": "2025-06-06T14:02:24.966142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(device)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "G = Generator(3, 3).to(device)\n",
    "F = Generator(3, 3).to(device)\n",
    "D_X = Discriminator(3).to(device)\n",
    "D_Y = Discriminator(3).to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_X = optim.Adam(D_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_Y = optim.Adam(D_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "\n",
    "lambda_cycle = 10.0\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    for i, (real_X, real_Y) in enumerate(zip(photo_loader, monet_loader)):\n",
    "        real_X = real_X.to(device)\n",
    "        real_Y = real_Y.to(device)\n",
    "        valid = torch.ones(real_X.size(0), 1, 30, 30).to(device)\n",
    "        fake = torch.zeros(real_X.size(0), 1, 30, 30).to(device)\n",
    "\n",
    "        # Generators\n",
    "        fake_Y = G(real_X)\n",
    "        rec_X = F(fake_Y)\n",
    "        fake_X = F(real_Y)\n",
    "        rec_Y = G(fake_X)\n",
    "\n",
    "        loss_G = criterion_GAN(D_Y(fake_Y), valid) + criterion_GAN(D_X(fake_X), valid)\n",
    "        loss_cycle = criterion_cycle(rec_X, real_X) + criterion_cycle(rec_Y, real_Y)\n",
    "        loss_total_G = loss_G + lambda_cycle * loss_cycle\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_total_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Discriminators\n",
    "        optimizer_D_X.zero_grad()\n",
    "        loss_D_X = criterion_GAN(D_X(real_X), valid) + criterion_GAN(D_X(fake_X.detach()), fake)\n",
    "        loss_D_X.backward()\n",
    "        optimizer_D_X.step()\n",
    "\n",
    "        optimizer_D_Y.zero_grad()\n",
    "        loss_D_Y = criterion_GAN(D_Y(real_Y), valid) + criterion_GAN(D_Y(fake_Y.detach()), fake)\n",
    "        loss_D_Y.backward()\n",
    "        optimizer_D_Y.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}: G_loss: {loss_total_G.item():.4f}, D_X: {loss_D_X.item():.4f}, D_Y: {loss_D_Y.item():.4f}\")"
   ],
   "id": "1a73521bf64ccaa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1, Batch 0: G_loss: 11.9555, D_X: 1.3480, D_Y: 1.5696\n",
      "Epoch 1, Batch 100: G_loss: 6.8848, D_X: 0.2377, D_Y: 0.2568\n",
      "Epoch 1, Batch 200: G_loss: 7.0345, D_X: 0.3929, D_Y: 0.4980\n",
      "Epoch 2, Batch 0: G_loss: 6.5519, D_X: 0.4686, D_Y: 0.3899\n",
      "Epoch 2, Batch 100: G_loss: 6.3739, D_X: 0.3846, D_Y: 0.7823\n",
      "Epoch 2, Batch 200: G_loss: 7.5516, D_X: 0.2855, D_Y: 0.4029\n",
      "Epoch 3, Batch 0: G_loss: 5.4740, D_X: 0.4405, D_Y: 0.3090\n",
      "Epoch 3, Batch 100: G_loss: 5.6143, D_X: 0.2801, D_Y: 0.5408\n",
      "Epoch 3, Batch 200: G_loss: 5.9411, D_X: 0.4042, D_Y: 0.4016\n",
      "Epoch 4, Batch 0: G_loss: 5.4786, D_X: 0.1599, D_Y: 0.3949\n",
      "Epoch 4, Batch 100: G_loss: 7.7105, D_X: 0.5595, D_Y: 0.4573\n",
      "Epoch 4, Batch 200: G_loss: 5.6536, D_X: 0.1406, D_Y: 0.7757\n",
      "Epoch 5, Batch 0: G_loss: 5.7694, D_X: 0.1026, D_Y: 0.2331\n",
      "Epoch 5, Batch 100: G_loss: 6.3901, D_X: 0.2870, D_Y: 1.0039\n",
      "Epoch 5, Batch 200: G_loss: 6.2900, D_X: 0.1968, D_Y: 0.4952\n",
      "Epoch 6, Batch 0: G_loss: 6.9517, D_X: 0.4389, D_Y: 0.4500\n",
      "Epoch 6, Batch 100: G_loss: 5.7269, D_X: 0.3253, D_Y: 0.2064\n",
      "Epoch 6, Batch 200: G_loss: 6.7816, D_X: 0.4121, D_Y: 0.6372\n",
      "Epoch 7, Batch 0: G_loss: 6.7365, D_X: 0.1366, D_Y: 0.1730\n",
      "Epoch 7, Batch 100: G_loss: 7.8320, D_X: 0.2226, D_Y: 0.1710\n",
      "Epoch 7, Batch 200: G_loss: 6.2612, D_X: 0.1454, D_Y: 0.2751\n",
      "Epoch 8, Batch 0: G_loss: 5.1131, D_X: 0.4109, D_Y: 0.2241\n",
      "Epoch 8, Batch 100: G_loss: 7.5909, D_X: 0.3668, D_Y: 0.3899\n",
      "Epoch 8, Batch 200: G_loss: 7.2666, D_X: 0.2854, D_Y: 0.2796\n",
      "Epoch 9, Batch 0: G_loss: 5.3823, D_X: 0.2345, D_Y: 0.1970\n",
      "Epoch 9, Batch 100: G_loss: 6.6674, D_X: 0.4095, D_Y: 0.4210\n",
      "Epoch 9, Batch 200: G_loss: 6.0848, D_X: 0.6104, D_Y: 0.6787\n",
      "Epoch 10, Batch 0: G_loss: 5.2753, D_X: 0.5628, D_Y: 0.3470\n",
      "Epoch 10, Batch 100: G_loss: 6.9953, D_X: 0.6371, D_Y: 0.7486\n",
      "Epoch 10, Batch 200: G_loss: 4.9652, D_X: 0.2254, D_Y: 0.3912\n",
      "Epoch 11, Batch 0: G_loss: 6.7069, D_X: 0.5134, D_Y: 0.3345\n",
      "Epoch 11, Batch 100: G_loss: 6.6530, D_X: 0.3348, D_Y: 0.1706\n",
      "Epoch 11, Batch 200: G_loss: 5.1076, D_X: 0.1873, D_Y: 0.1521\n",
      "Epoch 12, Batch 0: G_loss: 4.5728, D_X: 0.2986, D_Y: 0.0367\n",
      "Epoch 12, Batch 100: G_loss: 7.5155, D_X: 0.2079, D_Y: 0.1644\n",
      "Epoch 12, Batch 200: G_loss: 7.3001, D_X: 0.0554, D_Y: 0.6614\n",
      "Epoch 13, Batch 0: G_loss: 6.4064, D_X: 0.1307, D_Y: 0.3478\n",
      "Epoch 13, Batch 100: G_loss: 5.8329, D_X: 0.2646, D_Y: 0.0776\n",
      "Epoch 13, Batch 200: G_loss: 4.6968, D_X: 0.2392, D_Y: 0.5142\n",
      "Epoch 14, Batch 0: G_loss: 5.9332, D_X: 0.5987, D_Y: 0.2452\n",
      "Epoch 14, Batch 100: G_loss: 5.8975, D_X: 0.1076, D_Y: 0.5611\n",
      "Epoch 14, Batch 200: G_loss: 4.9838, D_X: 0.5753, D_Y: 0.4470\n",
      "Epoch 15, Batch 0: G_loss: 4.9737, D_X: 0.4739, D_Y: 0.2383\n",
      "Epoch 15, Batch 100: G_loss: 6.9252, D_X: 0.3766, D_Y: 0.1499\n",
      "Epoch 15, Batch 200: G_loss: 5.1346, D_X: 0.1701, D_Y: 0.1335\n",
      "Epoch 16, Batch 0: G_loss: 5.0087, D_X: 0.3710, D_Y: 0.1884\n",
      "Epoch 16, Batch 100: G_loss: 5.8886, D_X: 0.1926, D_Y: 0.0791\n",
      "Epoch 16, Batch 200: G_loss: 7.3239, D_X: 0.4050, D_Y: 0.0813\n",
      "Epoch 17, Batch 0: G_loss: 7.2920, D_X: 0.0280, D_Y: 0.1095\n",
      "Epoch 17, Batch 100: G_loss: 5.7220, D_X: 0.7082, D_Y: 0.1411\n",
      "Epoch 17, Batch 200: G_loss: 5.1825, D_X: 0.1473, D_Y: 0.1365\n",
      "Epoch 18, Batch 0: G_loss: 4.4026, D_X: 0.2736, D_Y: 0.1178\n",
      "Epoch 18, Batch 100: G_loss: 5.7155, D_X: 0.0385, D_Y: 0.0495\n",
      "Epoch 18, Batch 200: G_loss: 5.3638, D_X: 0.4380, D_Y: 0.2133\n",
      "Epoch 19, Batch 0: G_loss: 5.5865, D_X: 0.1161, D_Y: 0.3643\n",
      "Epoch 19, Batch 100: G_loss: 9.0849, D_X: 0.0769, D_Y: 0.2138\n",
      "Epoch 19, Batch 200: G_loss: 5.3097, D_X: 0.1465, D_Y: 0.1073\n",
      "Epoch 20, Batch 0: G_loss: 5.0986, D_X: 0.3709, D_Y: 0.1476\n",
      "Epoch 20, Batch 100: G_loss: 4.3302, D_X: 0.5032, D_Y: 0.2103\n",
      "Epoch 20, Batch 200: G_loss: 5.7628, D_X: 0.1570, D_Y: 0.0661\n",
      "Epoch 21, Batch 0: G_loss: 4.6645, D_X: 0.1415, D_Y: 0.2153\n",
      "Epoch 21, Batch 100: G_loss: 5.2484, D_X: 0.2248, D_Y: 0.0412\n",
      "Epoch 21, Batch 200: G_loss: 9.7295, D_X: 0.1237, D_Y: 0.1208\n",
      "Epoch 22, Batch 0: G_loss: 7.2781, D_X: 0.3296, D_Y: 0.0509\n",
      "Epoch 22, Batch 100: G_loss: 6.0082, D_X: 0.1085, D_Y: 0.0580\n",
      "Epoch 22, Batch 200: G_loss: 6.5892, D_X: 0.1239, D_Y: 0.0329\n",
      "Epoch 23, Batch 0: G_loss: 5.2270, D_X: 0.2212, D_Y: 0.1353\n",
      "Epoch 23, Batch 100: G_loss: 6.8545, D_X: 0.2633, D_Y: 0.6999\n",
      "Epoch 23, Batch 200: G_loss: 7.4320, D_X: 0.1786, D_Y: 0.0508\n",
      "Epoch 24, Batch 0: G_loss: 4.2824, D_X: 0.1484, D_Y: 0.3161\n",
      "Epoch 24, Batch 100: G_loss: 7.8434, D_X: 0.4729, D_Y: 0.0324\n",
      "Epoch 24, Batch 200: G_loss: 5.0253, D_X: 0.0940, D_Y: 0.2788\n",
      "Epoch 25, Batch 0: G_loss: 6.9273, D_X: 0.1433, D_Y: 0.0678\n",
      "Epoch 25, Batch 100: G_loss: 6.5232, D_X: 0.0609, D_Y: 0.1874\n",
      "Epoch 25, Batch 200: G_loss: 4.9417, D_X: 0.1414, D_Y: 0.1665\n",
      "Epoch 26, Batch 0: G_loss: 5.1669, D_X: 0.2086, D_Y: 0.0419\n",
      "Epoch 26, Batch 100: G_loss: 6.0977, D_X: 0.2393, D_Y: 0.0590\n",
      "Epoch 26, Batch 200: G_loss: 6.5961, D_X: 0.3583, D_Y: 0.0945\n",
      "Epoch 27, Batch 0: G_loss: 5.6207, D_X: 0.5273, D_Y: 0.2551\n",
      "Epoch 27, Batch 100: G_loss: 6.1454, D_X: 0.1540, D_Y: 0.0486\n",
      "Epoch 27, Batch 200: G_loss: 5.3126, D_X: 0.1151, D_Y: 0.2403\n",
      "Epoch 28, Batch 0: G_loss: 3.6694, D_X: 0.1930, D_Y: 0.4568\n",
      "Epoch 28, Batch 100: G_loss: 4.5804, D_X: 0.1125, D_Y: 0.1333\n",
      "Epoch 28, Batch 200: G_loss: 5.2337, D_X: 0.2417, D_Y: 0.1060\n",
      "Epoch 29, Batch 0: G_loss: 5.4704, D_X: 0.0415, D_Y: 0.0682\n",
      "Epoch 29, Batch 100: G_loss: 3.9053, D_X: 0.2011, D_Y: 0.2453\n",
      "Epoch 29, Batch 200: G_loss: 6.4908, D_X: 0.0927, D_Y: 0.1765\n",
      "Epoch 30, Batch 0: G_loss: 4.7339, D_X: 0.2439, D_Y: 0.1949\n",
      "Epoch 30, Batch 100: G_loss: 6.8027, D_X: 0.2631, D_Y: 0.1146\n",
      "Epoch 30, Batch 200: G_loss: 7.0174, D_X: 0.0997, D_Y: 0.1790\n",
      "Epoch 31, Batch 0: G_loss: 4.8294, D_X: 0.5750, D_Y: 0.1216\n",
      "Epoch 31, Batch 100: G_loss: 5.2250, D_X: 0.2678, D_Y: 0.1445\n",
      "Epoch 31, Batch 200: G_loss: 5.1566, D_X: 0.1157, D_Y: 0.1267\n",
      "Epoch 32, Batch 0: G_loss: 5.6527, D_X: 0.2506, D_Y: 0.1835\n",
      "Epoch 32, Batch 100: G_loss: 4.8936, D_X: 0.0990, D_Y: 0.0859\n",
      "Epoch 32, Batch 200: G_loss: 5.6958, D_X: 0.2691, D_Y: 0.1474\n",
      "Epoch 33, Batch 0: G_loss: 4.9800, D_X: 0.2577, D_Y: 0.0699\n",
      "Epoch 33, Batch 100: G_loss: 5.4160, D_X: 0.2053, D_Y: 0.2275\n",
      "Epoch 33, Batch 200: G_loss: 5.8210, D_X: 0.1543, D_Y: 0.0520\n",
      "Epoch 34, Batch 0: G_loss: 5.6986, D_X: 0.3364, D_Y: 0.0233\n",
      "Epoch 34, Batch 100: G_loss: 5.3643, D_X: 0.0907, D_Y: 0.1377\n",
      "Epoch 34, Batch 200: G_loss: 5.2276, D_X: 0.3049, D_Y: 0.3434\n",
      "Epoch 35, Batch 0: G_loss: 5.7846, D_X: 0.3550, D_Y: 0.1406\n",
      "Epoch 35, Batch 100: G_loss: 4.7558, D_X: 0.2340, D_Y: 0.1412\n",
      "Epoch 35, Batch 200: G_loss: 5.0240, D_X: 0.2499, D_Y: 0.2070\n",
      "Epoch 36, Batch 0: G_loss: 5.6380, D_X: 0.1737, D_Y: 0.0597\n",
      "Epoch 36, Batch 100: G_loss: 4.2874, D_X: 0.1080, D_Y: 0.1993\n",
      "Epoch 36, Batch 200: G_loss: 7.5139, D_X: 0.1303, D_Y: 0.0598\n",
      "Epoch 37, Batch 0: G_loss: 3.9829, D_X: 0.5083, D_Y: 0.0870\n",
      "Epoch 37, Batch 100: G_loss: 5.2014, D_X: 0.1425, D_Y: 0.1242\n",
      "Epoch 37, Batch 200: G_loss: 4.1521, D_X: 0.2175, D_Y: 0.0755\n",
      "Epoch 38, Batch 0: G_loss: 4.5046, D_X: 0.2801, D_Y: 0.0635\n",
      "Epoch 38, Batch 100: G_loss: 5.4378, D_X: 0.1145, D_Y: 0.1513\n",
      "Epoch 38, Batch 200: G_loss: 5.9670, D_X: 0.1132, D_Y: 0.1281\n",
      "Epoch 39, Batch 0: G_loss: 4.4860, D_X: 0.4604, D_Y: 0.2543\n",
      "Epoch 39, Batch 100: G_loss: 6.3551, D_X: 0.1885, D_Y: 0.0695\n",
      "Epoch 39, Batch 200: G_loss: 6.1973, D_X: 1.0270, D_Y: 0.2511\n",
      "Epoch 40, Batch 0: G_loss: 5.7998, D_X: 0.1977, D_Y: 0.0699\n",
      "Epoch 40, Batch 100: G_loss: 7.1358, D_X: 0.0849, D_Y: 0.1636\n",
      "Epoch 40, Batch 200: G_loss: 5.5089, D_X: 0.1406, D_Y: 0.2706\n",
      "Epoch 41, Batch 0: G_loss: 5.3636, D_X: 0.1414, D_Y: 0.2886\n",
      "Epoch 41, Batch 100: G_loss: 5.7071, D_X: 0.2009, D_Y: 0.0372\n",
      "Epoch 41, Batch 200: G_loss: 5.8883, D_X: 0.3324, D_Y: 0.0551\n",
      "Epoch 42, Batch 0: G_loss: 5.1892, D_X: 0.7406, D_Y: 0.0270\n",
      "Epoch 42, Batch 100: G_loss: 6.1138, D_X: 0.0529, D_Y: 0.0537\n",
      "Epoch 42, Batch 200: G_loss: 8.0072, D_X: 0.3476, D_Y: 0.1388\n",
      "Epoch 43, Batch 0: G_loss: 4.6666, D_X: 0.1079, D_Y: 0.1969\n",
      "Epoch 43, Batch 100: G_loss: 6.1513, D_X: 0.0893, D_Y: 0.0770\n",
      "Epoch 43, Batch 200: G_loss: 5.1001, D_X: 0.1320, D_Y: 0.1071\n",
      "Epoch 44, Batch 0: G_loss: 4.5797, D_X: 0.1940, D_Y: 0.0990\n",
      "Epoch 44, Batch 100: G_loss: 5.6893, D_X: 0.2938, D_Y: 0.0887\n",
      "Epoch 44, Batch 200: G_loss: 5.8403, D_X: 0.3540, D_Y: 0.1809\n",
      "Epoch 45, Batch 0: G_loss: 4.2790, D_X: 0.1098, D_Y: 0.4053\n",
      "Epoch 45, Batch 100: G_loss: 4.5626, D_X: 0.3528, D_Y: 0.1231\n",
      "Epoch 45, Batch 200: G_loss: 4.6512, D_X: 0.1562, D_Y: 0.1507\n",
      "Epoch 46, Batch 0: G_loss: 4.3918, D_X: 0.1036, D_Y: 0.0860\n",
      "Epoch 46, Batch 100: G_loss: 3.7263, D_X: 0.0607, D_Y: 0.1411\n",
      "Epoch 46, Batch 200: G_loss: 5.0648, D_X: 0.2700, D_Y: 0.1102\n",
      "Epoch 47, Batch 0: G_loss: 4.5173, D_X: 0.2580, D_Y: 0.0987\n",
      "Epoch 47, Batch 100: G_loss: 3.9722, D_X: 0.2185, D_Y: 0.3648\n",
      "Epoch 47, Batch 200: G_loss: 4.7334, D_X: 0.4105, D_Y: 0.7542\n",
      "Epoch 48, Batch 0: G_loss: 5.6297, D_X: 0.3485, D_Y: 0.0680\n",
      "Epoch 48, Batch 100: G_loss: 4.7105, D_X: 0.3490, D_Y: 0.0803\n",
      "Epoch 48, Batch 200: G_loss: 4.7811, D_X: 0.4193, D_Y: 0.0410\n",
      "Epoch 49, Batch 0: G_loss: 5.1907, D_X: 0.5415, D_Y: 0.2324\n",
      "Epoch 49, Batch 100: G_loss: 4.8940, D_X: 0.2061, D_Y: 0.1728\n",
      "Epoch 49, Batch 200: G_loss: 4.3131, D_X: 0.1783, D_Y: 0.1736\n",
      "Epoch 50, Batch 0: G_loss: 4.9034, D_X: 0.1167, D_Y: 0.1579\n",
      "Epoch 50, Batch 100: G_loss: 3.3848, D_X: 0.7425, D_Y: 0.0795\n",
      "Epoch 50, Batch 200: G_loss: 4.8973, D_X: 0.1444, D_Y: 0.1060\n",
      "Epoch 51, Batch 0: G_loss: 4.7717, D_X: 0.2040, D_Y: 0.2285\n",
      "Epoch 51, Batch 100: G_loss: 5.5889, D_X: 0.2128, D_Y: 0.2077\n",
      "Epoch 51, Batch 200: G_loss: 5.9145, D_X: 0.1249, D_Y: 0.0508\n",
      "Epoch 52, Batch 0: G_loss: 5.0032, D_X: 0.2383, D_Y: 0.0715\n",
      "Epoch 52, Batch 100: G_loss: 5.0631, D_X: 0.2091, D_Y: 0.0678\n",
      "Epoch 52, Batch 200: G_loss: 4.9415, D_X: 0.1182, D_Y: 0.1068\n",
      "Epoch 53, Batch 0: G_loss: 4.9915, D_X: 0.0852, D_Y: 0.1431\n",
      "Epoch 53, Batch 100: G_loss: 4.6580, D_X: 0.1071, D_Y: 0.0670\n",
      "Epoch 53, Batch 200: G_loss: 3.4305, D_X: 0.0948, D_Y: 0.3969\n",
      "Epoch 54, Batch 0: G_loss: 4.7564, D_X: 0.2104, D_Y: 0.1108\n",
      "Epoch 54, Batch 100: G_loss: 3.9554, D_X: 0.1287, D_Y: 0.2252\n",
      "Epoch 54, Batch 200: G_loss: 5.1533, D_X: 0.3625, D_Y: 0.1013\n",
      "Epoch 55, Batch 0: G_loss: 4.8255, D_X: 0.0591, D_Y: 0.0486\n",
      "Epoch 55, Batch 100: G_loss: 4.6657, D_X: 0.2014, D_Y: 0.1122\n",
      "Epoch 55, Batch 200: G_loss: 5.2083, D_X: 0.1093, D_Y: 0.1346\n",
      "Epoch 56, Batch 0: G_loss: 3.4959, D_X: 0.7678, D_Y: 0.1333\n",
      "Epoch 56, Batch 100: G_loss: 4.3834, D_X: 0.1260, D_Y: 0.0743\n",
      "Epoch 56, Batch 200: G_loss: 5.8603, D_X: 0.1757, D_Y: 0.0630\n",
      "Epoch 57, Batch 0: G_loss: 5.9865, D_X: 0.1346, D_Y: 0.1444\n",
      "Epoch 57, Batch 100: G_loss: 4.4241, D_X: 0.2088, D_Y: 0.0418\n",
      "Epoch 57, Batch 200: G_loss: 4.8316, D_X: 0.1305, D_Y: 0.2918\n",
      "Epoch 58, Batch 0: G_loss: 4.8737, D_X: 0.1799, D_Y: 0.1229\n",
      "Epoch 58, Batch 100: G_loss: 6.2424, D_X: 0.3224, D_Y: 0.1588\n",
      "Epoch 58, Batch 200: G_loss: 3.9316, D_X: 0.5313, D_Y: 0.2608\n",
      "Epoch 59, Batch 0: G_loss: 3.6286, D_X: 0.3502, D_Y: 0.0898\n",
      "Epoch 59, Batch 100: G_loss: 4.0812, D_X: 0.1116, D_Y: 0.1120\n",
      "Epoch 59, Batch 200: G_loss: 6.5325, D_X: 0.1149, D_Y: 0.1982\n",
      "Epoch 60, Batch 0: G_loss: 5.3540, D_X: 0.1655, D_Y: 0.0767\n",
      "Epoch 60, Batch 100: G_loss: 4.4814, D_X: 0.1972, D_Y: 0.1221\n",
      "Epoch 60, Batch 200: G_loss: 4.4320, D_X: 0.0728, D_Y: 0.0525\n",
      "Epoch 61, Batch 0: G_loss: 5.2721, D_X: 0.1614, D_Y: 0.0601\n",
      "Epoch 61, Batch 100: G_loss: 5.1472, D_X: 0.1740, D_Y: 0.1512\n",
      "Epoch 61, Batch 200: G_loss: 5.1473, D_X: 0.4276, D_Y: 0.0429\n",
      "Epoch 62, Batch 0: G_loss: 5.2312, D_X: 0.0604, D_Y: 0.1457\n",
      "Epoch 62, Batch 100: G_loss: 5.0788, D_X: 0.1068, D_Y: 0.1015\n",
      "Epoch 62, Batch 200: G_loss: 5.1124, D_X: 0.2151, D_Y: 0.2356\n",
      "Epoch 63, Batch 0: G_loss: 4.4287, D_X: 0.3081, D_Y: 0.2953\n",
      "Epoch 63, Batch 100: G_loss: 5.4684, D_X: 0.0361, D_Y: 0.0893\n",
      "Epoch 63, Batch 200: G_loss: 5.0739, D_X: 0.1222, D_Y: 0.0995\n",
      "Epoch 64, Batch 0: G_loss: 3.9631, D_X: 0.5159, D_Y: 0.0807\n",
      "Epoch 64, Batch 100: G_loss: 7.1090, D_X: 0.0634, D_Y: 0.0453\n",
      "Epoch 64, Batch 200: G_loss: 5.6107, D_X: 0.1027, D_Y: 0.1220\n",
      "Epoch 65, Batch 0: G_loss: 4.3581, D_X: 0.2605, D_Y: 0.0431\n",
      "Epoch 65, Batch 100: G_loss: 4.8827, D_X: 0.1297, D_Y: 0.0785\n",
      "Epoch 65, Batch 200: G_loss: 4.6639, D_X: 0.2962, D_Y: 0.2135\n",
      "Epoch 66, Batch 0: G_loss: 4.4333, D_X: 0.0317, D_Y: 0.1800\n",
      "Epoch 66, Batch 100: G_loss: 4.6270, D_X: 0.1565, D_Y: 0.0537\n",
      "Epoch 66, Batch 200: G_loss: 5.1211, D_X: 0.1194, D_Y: 0.1306\n",
      "Epoch 67, Batch 0: G_loss: 6.0594, D_X: 0.0868, D_Y: 0.0638\n",
      "Epoch 67, Batch 100: G_loss: 5.0802, D_X: 0.1326, D_Y: 0.0600\n",
      "Epoch 67, Batch 200: G_loss: 4.9089, D_X: 0.3698, D_Y: 0.0784\n",
      "Epoch 68, Batch 0: G_loss: 5.0607, D_X: 0.1148, D_Y: 0.0438\n",
      "Epoch 68, Batch 100: G_loss: 6.0563, D_X: 0.1047, D_Y: 0.0972\n",
      "Epoch 68, Batch 200: G_loss: 5.9555, D_X: 0.2612, D_Y: 0.1532\n",
      "Epoch 69, Batch 0: G_loss: 5.2838, D_X: 0.1923, D_Y: 0.2094\n",
      "Epoch 69, Batch 100: G_loss: 5.1689, D_X: 0.3674, D_Y: 0.0239\n",
      "Epoch 69, Batch 200: G_loss: 4.9773, D_X: 0.1099, D_Y: 0.0993\n",
      "Epoch 70, Batch 0: G_loss: 4.9762, D_X: 0.1042, D_Y: 0.0921\n",
      "Epoch 70, Batch 100: G_loss: 4.2483, D_X: 0.1726, D_Y: 0.0969\n",
      "Epoch 70, Batch 200: G_loss: 4.6134, D_X: 0.1058, D_Y: 0.1439\n",
      "Epoch 71, Batch 0: G_loss: 6.2457, D_X: 0.1700, D_Y: 0.1308\n",
      "Epoch 71, Batch 100: G_loss: 5.7001, D_X: 0.3425, D_Y: 0.0865\n",
      "Epoch 71, Batch 200: G_loss: 5.4198, D_X: 0.1450, D_Y: 0.0619\n",
      "Epoch 72, Batch 0: G_loss: 4.2801, D_X: 0.1272, D_Y: 0.2386\n",
      "Epoch 72, Batch 100: G_loss: 4.1473, D_X: 0.5652, D_Y: 0.0710\n",
      "Epoch 72, Batch 200: G_loss: 5.7673, D_X: 0.1667, D_Y: 0.0521\n",
      "Epoch 73, Batch 0: G_loss: 4.8922, D_X: 0.1974, D_Y: 0.2120\n",
      "Epoch 73, Batch 100: G_loss: 3.2578, D_X: 0.3116, D_Y: 0.0760\n",
      "Epoch 73, Batch 200: G_loss: 3.8393, D_X: 0.0773, D_Y: 0.0904\n",
      "Epoch 74, Batch 0: G_loss: 3.5560, D_X: 0.3189, D_Y: 0.0689\n",
      "Epoch 74, Batch 100: G_loss: 6.2057, D_X: 0.1535, D_Y: 0.0284\n",
      "Epoch 74, Batch 200: G_loss: 6.3093, D_X: 0.2428, D_Y: 0.0438\n",
      "Epoch 75, Batch 0: G_loss: 5.6155, D_X: 0.0667, D_Y: 0.1558\n",
      "Epoch 75, Batch 100: G_loss: 4.0358, D_X: 0.1143, D_Y: 0.3628\n",
      "Epoch 75, Batch 200: G_loss: 5.5337, D_X: 0.1155, D_Y: 0.1006\n",
      "Epoch 76, Batch 0: G_loss: 4.2620, D_X: 0.1496, D_Y: 0.0692\n",
      "Epoch 76, Batch 100: G_loss: 5.1492, D_X: 0.2557, D_Y: 0.2639\n",
      "Epoch 76, Batch 200: G_loss: 4.9203, D_X: 0.3630, D_Y: 0.1720\n",
      "Epoch 77, Batch 0: G_loss: 5.1738, D_X: 0.3749, D_Y: 0.1193\n",
      "Epoch 77, Batch 100: G_loss: 4.3343, D_X: 0.3551, D_Y: 0.1630\n",
      "Epoch 77, Batch 200: G_loss: 5.0657, D_X: 0.5067, D_Y: 0.1556\n",
      "Epoch 78, Batch 0: G_loss: 4.6408, D_X: 0.3167, D_Y: 0.0730\n",
      "Epoch 78, Batch 100: G_loss: 4.4257, D_X: 0.1269, D_Y: 0.0751\n",
      "Epoch 78, Batch 200: G_loss: 3.8801, D_X: 0.2244, D_Y: 0.0952\n",
      "Epoch 79, Batch 0: G_loss: 6.0619, D_X: 0.4440, D_Y: 0.1004\n",
      "Epoch 79, Batch 100: G_loss: 5.4298, D_X: 0.3882, D_Y: 0.0343\n",
      "Epoch 79, Batch 200: G_loss: 3.5893, D_X: 0.2214, D_Y: 0.1508\n",
      "Epoch 80, Batch 0: G_loss: 6.0197, D_X: 0.3359, D_Y: 0.0351\n",
      "Epoch 80, Batch 100: G_loss: 4.2179, D_X: 0.2262, D_Y: 0.0532\n",
      "Epoch 80, Batch 200: G_loss: 4.6664, D_X: 0.1441, D_Y: 0.1754\n",
      "Epoch 81, Batch 0: G_loss: 3.7568, D_X: 0.3458, D_Y: 0.7379\n",
      "Epoch 81, Batch 100: G_loss: 5.0512, D_X: 0.1322, D_Y: 0.2661\n",
      "Epoch 81, Batch 200: G_loss: 4.3937, D_X: 0.3093, D_Y: 0.3946\n",
      "Epoch 82, Batch 0: G_loss: 4.3566, D_X: 0.0660, D_Y: 0.2260\n",
      "Epoch 82, Batch 100: G_loss: 5.5656, D_X: 0.0707, D_Y: 0.0973\n",
      "Epoch 82, Batch 200: G_loss: 4.4933, D_X: 0.1867, D_Y: 0.2542\n",
      "Epoch 83, Batch 0: G_loss: 6.0520, D_X: 0.1483, D_Y: 0.0916\n",
      "Epoch 83, Batch 100: G_loss: 6.0656, D_X: 0.1910, D_Y: 0.3584\n",
      "Epoch 83, Batch 200: G_loss: 4.9024, D_X: 0.0904, D_Y: 0.0902\n",
      "Epoch 84, Batch 0: G_loss: 5.4461, D_X: 0.0813, D_Y: 0.0682\n",
      "Epoch 84, Batch 100: G_loss: 3.6440, D_X: 0.1407, D_Y: 0.0460\n",
      "Epoch 84, Batch 200: G_loss: 5.3886, D_X: 0.3014, D_Y: 0.1055\n",
      "Epoch 85, Batch 0: G_loss: 4.7028, D_X: 0.2681, D_Y: 0.2076\n",
      "Epoch 85, Batch 100: G_loss: 5.4748, D_X: 0.0809, D_Y: 0.0244\n",
      "Epoch 85, Batch 200: G_loss: 4.1178, D_X: 0.1880, D_Y: 0.2505\n",
      "Epoch 86, Batch 0: G_loss: 7.0196, D_X: 0.0514, D_Y: 0.0365\n",
      "Epoch 86, Batch 100: G_loss: 4.7469, D_X: 0.1705, D_Y: 0.0344\n",
      "Epoch 86, Batch 200: G_loss: 6.1743, D_X: 0.0660, D_Y: 0.0965\n",
      "Epoch 87, Batch 0: G_loss: 5.1271, D_X: 0.0862, D_Y: 0.1180\n",
      "Epoch 87, Batch 100: G_loss: 4.0918, D_X: 0.3608, D_Y: 0.0507\n",
      "Epoch 87, Batch 200: G_loss: 4.7943, D_X: 0.0568, D_Y: 0.1586\n",
      "Epoch 88, Batch 0: G_loss: 4.5290, D_X: 0.1995, D_Y: 0.1573\n",
      "Epoch 88, Batch 100: G_loss: 4.7251, D_X: 0.3407, D_Y: 0.0951\n",
      "Epoch 88, Batch 200: G_loss: 5.3265, D_X: 0.0457, D_Y: 0.2031\n",
      "Epoch 89, Batch 0: G_loss: 4.4205, D_X: 0.2705, D_Y: 0.0652\n",
      "Epoch 89, Batch 100: G_loss: 5.5134, D_X: 0.3037, D_Y: 0.2116\n",
      "Epoch 89, Batch 200: G_loss: 4.4772, D_X: 0.0381, D_Y: 0.1219\n",
      "Epoch 90, Batch 0: G_loss: 4.1187, D_X: 0.0451, D_Y: 0.0485\n",
      "Epoch 90, Batch 100: G_loss: 4.2398, D_X: 0.1897, D_Y: 0.1261\n",
      "Epoch 90, Batch 200: G_loss: 6.2321, D_X: 0.4117, D_Y: 0.0769\n",
      "Epoch 91, Batch 0: G_loss: 4.7315, D_X: 0.0740, D_Y: 0.0646\n",
      "Epoch 91, Batch 100: G_loss: 5.0549, D_X: 0.2131, D_Y: 0.1028\n",
      "Epoch 91, Batch 200: G_loss: 6.2233, D_X: 0.1893, D_Y: 0.0931\n",
      "Epoch 92, Batch 0: G_loss: 4.7742, D_X: 0.0676, D_Y: 0.0932\n",
      "Epoch 92, Batch 100: G_loss: 4.9425, D_X: 0.1712, D_Y: 0.0490\n",
      "Epoch 92, Batch 200: G_loss: 4.3571, D_X: 0.1427, D_Y: 0.0448\n",
      "Epoch 93, Batch 0: G_loss: 3.8178, D_X: 0.4307, D_Y: 0.0767\n",
      "Epoch 93, Batch 100: G_loss: 4.6847, D_X: 0.1095, D_Y: 0.1022\n",
      "Epoch 93, Batch 200: G_loss: 4.4798, D_X: 0.1326, D_Y: 0.0300\n",
      "Epoch 94, Batch 0: G_loss: 5.5190, D_X: 0.1992, D_Y: 0.0625\n",
      "Epoch 94, Batch 100: G_loss: 4.4742, D_X: 0.1588, D_Y: 0.0430\n",
      "Epoch 94, Batch 200: G_loss: 4.7424, D_X: 0.1421, D_Y: 0.0489\n",
      "Epoch 95, Batch 0: G_loss: 4.7779, D_X: 0.2629, D_Y: 0.0612\n",
      "Epoch 95, Batch 100: G_loss: 5.0693, D_X: 0.1163, D_Y: 0.0401\n",
      "Epoch 95, Batch 200: G_loss: 5.6054, D_X: 0.3179, D_Y: 0.0587\n",
      "Epoch 96, Batch 0: G_loss: 5.1341, D_X: 0.3115, D_Y: 0.0614\n",
      "Epoch 96, Batch 100: G_loss: 4.2072, D_X: 0.1652, D_Y: 0.0626\n",
      "Epoch 96, Batch 200: G_loss: 7.7757, D_X: 0.0794, D_Y: 0.0385\n",
      "Epoch 97, Batch 0: G_loss: 5.1916, D_X: 0.1065, D_Y: 0.0778\n",
      "Epoch 97, Batch 100: G_loss: 4.6257, D_X: 0.2306, D_Y: 0.0684\n",
      "Epoch 97, Batch 200: G_loss: 4.8890, D_X: 0.1921, D_Y: 0.1250\n",
      "Epoch 98, Batch 0: G_loss: 5.3405, D_X: 0.1795, D_Y: 0.0586\n",
      "Epoch 98, Batch 100: G_loss: 5.4266, D_X: 0.3557, D_Y: 0.0489\n",
      "Epoch 98, Batch 200: G_loss: 5.2462, D_X: 0.3040, D_Y: 0.1444\n",
      "Epoch 99, Batch 0: G_loss: 5.2817, D_X: 0.0698, D_Y: 0.0264\n",
      "Epoch 99, Batch 100: G_loss: 5.2332, D_X: 0.1944, D_Y: 0.0319\n",
      "Epoch 99, Batch 200: G_loss: 5.2781, D_X: 0.1635, D_Y: 0.0465\n",
      "Epoch 100, Batch 0: G_loss: 4.4222, D_X: 0.1409, D_Y: 0.0545\n",
      "Epoch 100, Batch 100: G_loss: 4.0875, D_X: 0.1037, D_Y: 0.0787\n",
      "Epoch 100, Batch 200: G_loss: 4.6191, D_X: 0.0715, D_Y: 0.0769\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T18:13:32.811904Z",
     "start_time": "2025-06-06T18:09:22.467494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "with zipfile.ZipFile(\"images.zip\", mode=\"w\") as zipf:\n",
    "    G.eval()\n",
    "    for i, img in enumerate(photo_loader):\n",
    "        if i >= 7000:\n",
    "            break\n",
    "        img = img.to(device)\n",
    "        with torch.no_grad():\n",
    "            fake = G(img)\n",
    "        fake = (fake.squeeze(0).cpu() + 1) / 2\n",
    "\n",
    "        buffer = io.BytesIO()\n",
    "        save_image(fake, buffer, format='JPEG')\n",
    "        buffer.seek(0)\n",
    "\n",
    "        zipf.writestr(f\"{i}.jpg\", buffer.read())"
   ],
   "id": "1859f991e3ba1ddf",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "42151b85baa81aec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
